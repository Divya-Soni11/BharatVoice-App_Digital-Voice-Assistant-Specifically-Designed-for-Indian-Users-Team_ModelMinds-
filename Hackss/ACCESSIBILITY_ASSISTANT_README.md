# ğŸ™ï¸ Voice-Controlled Accessibility Assistant

A powerful Android app that helps users navigate other applications using voice commands. Built with
on-device AI for complete privacy.

## ğŸŒŸ Features

### Core Capabilities

- **ğŸ“± Screen Reading**: Access and understand UI elements from any app
- **ğŸ—£ï¸ Voice Commands**: Navigate apps hands-free with natural language
- **ğŸ¤– AI-Powered**: Intelligent command interpretation using on-device LLM
- **ğŸ”’ Privacy-First**: All processing happens locally on your device
- **ğŸ¯ Accessibility**: Helps visually impaired and hands-free users

### Supported Actions

- **Click** buttons and interactive elements
- **Scroll** up and down through content
- **Read** screen content aloud
- **Type** text into fields
- **Describe** what's currently on screen
- **Navigate** between apps

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Voice Input    â”‚ â† User speaks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Speech Recognitionâ”‚ (Android SpeechRecognizer)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Processor   â”‚ â† Interprets command + screen context
â”‚ (RunAnywhere SDK)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Accessibility Service          â”‚
â”‚  â€¢ Reads UI from other apps     â”‚
â”‚  â€¢ Performs actions (click, etc)â”‚
â”‚  â€¢ Monitors screen changes      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Target App     â”‚ â† User's destination app
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Technology Stack

- **Android Accessibility Service API**: Screen reading and interaction
- **Android Speech Recognition**: Voice-to-text conversion
- **Text-to-Speech (TTS)**: Voice feedback to user
- **RunAnywhere SDK**: On-device LLM for command interpretation
- **Jetpack Compose**: Modern UI
- **Kotlin Coroutines**: Asynchronous operations
- **MVVM Architecture**: Clean separation of concerns

## ğŸš€ Getting Started

### Prerequisites

- Android device with API 24+ (Android 7.0+)
- ~500 MB free storage (for AI model)
- Microphone access
- Accessibility service permissions

### Installation

1. **Build and Install**
   ```bash
   cd Hackss
   ./gradlew assembleDebug
   adb install app/build/outputs/apk/debug/app-debug.apk
   ```

2. **Enable Accessibility Service**
    - Open the app
    - Tap "Enable" on the Accessibility Service card
    - Or: Settings â†’ Accessibility â†’ [App Name] â†’ Toggle ON
    - Grant permission

3. **Grant Microphone Permission**
    - The app will request this automatically
    - Or: Settings â†’ Apps â†’ [App Name] â†’ Permissions â†’ Microphone

4. **Download AI Model**
    - Go to "Chat" tab
    - Tap "Models"
    - Download "SmolLM2 360M Q8_0" (recommended, 119 MB)
    - Tap "Load" to activate

## ğŸ¯ Usage

### Basic Workflow

1. **Launch the App**
    - Open the app
    - Switch to "Assistant" tab
    - Verify Accessibility Service is enabled (green checkmark)

2. **Open Target App**
    - Navigate to any app you want to control
    - The assistant monitors screen in background

3. **Give Voice Commands**
    - Return to assistant app (or use from notification)
    - Tap microphone button
    - Speak your command
    - Wait for confirmation

### Example Commands

#### Reading Content

```
"What's on this screen?"
"Read the screen"
"What do I see here?"
```

**Response**: AI describes visible elements

#### Clicking Elements

```
"Click the submit button"
"Tap on login"
"Press the menu icon"
```

**Action**: Finds and clicks the specified element

#### Scrolling

```
"Scroll down"
"Scroll up"
"Go down"
```

**Action**: Scrolls the current view

#### Typing Text

```
"Type hello world"
"Enter my email"
"Type search query"
```

**Action**: Types into focused text field

## ğŸ›ï¸ Project Structure

```
Hackss/app/src/main/java/com/runanywhere/startup_hackathon20/
â”‚
â”œâ”€â”€ accessibility/
â”‚   â”œâ”€â”€ AccessibilityAssistantService.kt  # Core service for screen reading
â”‚   â”œâ”€â”€ UIAnalyzer.kt                     # Extracts UI elements
â”‚   â”œâ”€â”€ ScreenStateManager.kt             # Stores current screen state
â”‚   â””â”€â”€ [Data classes]
â”‚
â”œâ”€â”€ voice/
â”‚   â””â”€â”€ VoiceAssistant.kt                 # Speech recognition + TTS
â”‚
â”œâ”€â”€ ai/
â”‚   â””â”€â”€ AICommandProcessor.kt             # LLM-powered command interpretation
â”‚
â”œâ”€â”€ AssistantViewModel.kt                 # Coordinates all components
â”œâ”€â”€ AssistantScreen.kt                    # Main UI for voice assistant
â”œâ”€â”€ MainActivity.kt                       # App entry point
â””â”€â”€ [Other files...]
```

## ğŸ”§ Configuration

### Accessibility Service Config

`app/src/main/res/xml/accessibility_service_config.xml`

```xml
<accessibility-service
    android:accessibilityEventTypes="typeAllMask"
    android:accessibilityFeedbackType="feedbackGeneric"
    android:canRetrieveWindowContent="true"
    android:packageNames="@null"  <!-- null = all apps -->
    ... />
```

### Permissions Required

`app/src/main/AndroidManifest.xml`

```xml
<uses-permission android:name="android.permission.INTERNET" />
<uses-permission android:name="android.permission.RECORD_AUDIO" />
<uses-permission android:name="android.permission.FOREGROUND_SERVICE" />
<uses-permission android:name="android.permission.POST_NOTIFICATIONS" />
```

## ğŸ¨ UI Components

### Main Screen

- **Service Status Card**: Shows if accessibility is enabled
- **Microphone Button**: Large FAB with animation when listening
- **Status Display**: Shows current command and response
- **Commands Help**: Expandable list of example commands

### States

- ğŸ”´ **Disabled**: Accessibility service not enabled
- ğŸŸ¢ **Ready**: All permissions granted, ready to listen
- ğŸ”µ **Listening**: Actively recording voice
- ğŸŸ¡ **Processing**: AI interpreting command
- âš« **Executing**: Performing action

## ğŸ§  How It Works

### 1. Screen Analysis

```kotlin
// Accessibility service captures UI hierarchy
val rootNode = rootInActiveWindow
val screenData = uiAnalyzer.extractScreen(rootNode)

// Extract elements
screenData.elements.forEach { element ->
    println("${element.text} [${element.isClickable}]")
}
```

### 2. Voice Command Processing

```kotlin
// User speaks â†’ Speech recognizer converts to text
voiceAssistant.startListening { command ->
    // "Click the submit button"
    processCommand(command)
}
```

### 3. AI Interpretation

```kotlin
// AI analyzes command + screen context
val response = aiProcessor.interpretCommand(
    userCommand = "Click the submit button",
    screenData = currentScreen
)

// Response: 
// { action: "click", targetElement: "Submit" }
```

### 4. Action Execution

```kotlin
// Perform the action via Accessibility Service
when (response.action) {
    CLICK -> {
        service.clickElementByText(response.targetElement)
    }
    SCROLL -> {
        service.scroll(response.direction)
    }
    // ... etc
}
```

## ğŸ” Privacy & Security

### âœ… Privacy Features

- **No data collection**: Nothing sent to servers
- **On-device AI**: All processing local
- **User control**: Explicit permission required
- **Transparent**: User sees all actions

### âš ï¸ Important Notes

- **Cannot read passwords**: Input fields marked as sensitive are hidden
- **Banking apps**: Some apps block accessibility for security
- **App restrictions**: Developers can prevent accessibility access

## ğŸ¯ Use Cases

### Primary Use Cases

1. **Visually Impaired Users**: Navigate apps without seeing screen
2. **Motor Impairments**: Control apps without touch
3. **Hands-Free Operation**: Use phone while cooking, driving, etc.
4. **Elderly Users**: Simplify complex interfaces
5. **Power Users**: Automate repetitive tasks

### Example Scenarios

**Scenario 1: Cooking**

- User follows recipe on phone
- Hands are messy/wet
- Says "Scroll down" to continue reading

**Scenario 2: Accessibility**

- Visually impaired user opens Instagram
- Says "What's on this screen?"
- AI: "You're on Instagram feed. I see 5 posts..."
- User: "Click the first post"

**Scenario 3: Multitasking**

- User working on laptop
- Says "Type meeting notes" into phone
- Continues working without touching phone

## ğŸš§ Known Limitations

1. **Some apps block accessibility**: Banking, secure apps
2. **Accuracy depends on UI quality**: Poorly labeled UIs are harder
3. **Battery usage**: Voice recognition and AI use power
4. **Language**: Currently optimized for English
5. **Complex gestures**: Can't do pinch-to-zoom, double-tap, etc.

## ğŸ”® Future Enhancements

### Planned Features

- [ ] **Wake word detection** ("Hey Assistant...")
- [ ] **Continuous listening mode**
- [ ] **Custom voice shortcuts**
- [ ] **Multi-language support**
- [ ] **OCR for images** (read text from images)
- [ ] **Gesture support** (swipe, pinch)
- [ ] **Learning mode** (train on your vocabulary)
- [ ] **App-specific profiles** (custom commands per app)

### Advanced Ideas

- **Screen recording + playback**: Automate workflows
- **Voice-controlled phone settings**: "Turn on WiFi"
- **Integration with smart home**: "Show security camera"
- **Collaborative features**: Share voice shortcuts

## ğŸ› Troubleshooting

### Accessibility Service Won't Enable

- **Solution**: Check Settings â†’ Accessibility, enable manually
- **Cause**: Some Android versions require manual activation

### Voice Recognition Not Working

- **Check**: Microphone permission granted?
- **Check**: Is microphone physically blocked?
- **Try**: Speak more clearly, reduce background noise

### Commands Not Executing

- **Check**: Is accessibility service still enabled?
- **Check**: Is target app blocking accessibility?
- **Try**: Re-enable accessibility service
- **Debug**: Check logcat for errors (`adb logcat | grep Accessibility`)

### AI Responses Are Wrong

- **Solution**: Use more specific commands
- **Solution**: Describe elements by their exact text
- **Note**: Fallback to rule-based if LLM not loaded

### App Crashes

- **Check**: Sufficient memory? Try smaller AI model
- **Check**: Device API level 24+?
- **Report**: Share logcat output

## ğŸ“š Technical Deep Dive

### Accessibility Service Lifecycle

```
onCreate() â†’ onServiceConnected() â†’ onAccessibilityEvent() â†’ onDestroy()
```

### Event Flow

```
User opens app â†’ TYPE_WINDOW_STATE_CHANGED
User scrolls â†’ TYPE_VIEW_SCROLLED
Button appears â†’ TYPE_WINDOW_CONTENT_CHANGED
```

### Memory Management

- UI nodes are recycled after use
- Screen history limited to 10 items
- LLM loads on-demand

## ğŸ¤ Contributing

This is a hackathon project, but contributions welcome!

### Areas to Improve

- Better AI prompt engineering
- More robust element matching
- Additional languages
- UI enhancements
- Performance optimization

## ğŸ“„ License

See main project LICENSE file.

## ğŸ™ Acknowledgments

- **RunAnywhere SDK**: On-device LLM inference
- **Android Accessibility API**: Core functionality
- **Google Speech Services**: Voice recognition

## ğŸ“ Support

For issues:

1. Check troubleshooting section
2. Review logcat logs
3. Open GitHub issue with details

---

**Built with â¤ï¸ for CGC Hackathon**

*Making technology accessible to everyone, everywhere.*
